---
layout: post
title: MixConv：Mixed Depthwise Convolutional Kernels
category: papers
tags: [Deep learning]
comments: true
---

# MixConv：Mixed Depthwise Convolutional Kernels

Original paper: https://arxiv.org/pdf/1907.09595.pdf

Authors: Mingxing Tan, Quoc V. Le (Google brain)

- 참고 글
  - https://www.youtube.com/watch?v=252YxqpHzsg&feature=youtu.be
  - https://www.slideshare.net/JinwonLee9/pr183-mixnet-mixed-depthwise-convolutional-kernels
  - https://medium.com/@tantara/mixnet-on-tensorflow-lite-94520a89b791

## Introduction
- 최근 convolution network design의 트렌드는 네트워크의 정확도와 효율을 모두 높히는 방향으로 연구가 활발히 진행되고 있음
- 이로 인해 다양한 종류의 depthwise convolution들이 연구되어 최근의 CNN모델들에 적용되고 있는 추세
  - MobileNets, ShuffleNets, NASNets, AmoebaNet, MnasNet, EfficientNet 등등
  - 대부분의 모델들에서 depthwise convolution들을 사용함
- 예전의 CNN들은 주로 3x3 conv layer들을 많이 사용했지만, 최근엔 5x5나 7x7같이 큰 커널 사이즈를 갖는 conv layer들을 많이 사용함
  - __큰 커널을 사용하는것이 모델의 정확도나 효율 관점에서 더 좋다는 연구결과들이 있음__

- 본 논문에선 __"큰 커널을 사용할수록 정확도가 향상되나?"__ 라는 fundamental한 질문을 던졌음
  - Conv layer에서 큰 크기의 커널을 사용할경우 파라미터나 연산량이 증가하더라도 더 디테일한 high-resolution pattern들을 학습할 수 있게 됨
  - 반면 작은 크기의 커널을 사용할 경우 low-resolution 패턴을 학습하게 됨
  - 이로 인해 큰 크기의 커널을 사용할수록 네트워크가 더 정확하게 동작 할 수 있게 됨
  - 저자들은 커널 크기가 계속 커질수록 정확도도 계속 높아질것인가에 대한 질문을 한 것임

<center>
<figure>
<img src="/assets/post_img/papers/2019-11-15-mixnet/fig1.PNG" alt="views">
<figcaption></figcaption>
</figure>
</center>

- 위 그림에서 왼쪽은 MobileNet v1의 ImageNet Top-1 정확도 결과를, 오른쪽은 MobileNet v2의 정확도 결과를 보여줌
  - x축은 커널 크기, y축은 정확도를 의미
- 그림에서 보이는것처럼 커널 크기가 커지더라도 일정 수준 이상으로 커지게 되면 오히려 정확도가 감소하게 되는 것을 확인 할 수 있음
  - MobileNet v1의 경우 7x7 커널 크기를 썼을 때 가장 성능이 좋았으며, v2의 경우 9x9가 가장 이상적인 커널 크기

- 커널의 크기가 input resolution까지 커지는 극단적인 경우를 생각해보면 이는 마치 fully-connected network처럼 동작하는것과 같게 됨
  - 입력 영상의 각 픽셀(값)과 커널 파라미터가 1:1 매칭되기때문에 fc layer와 동일함
  - __따라서 커널 크기가 extreme하게 계속 커지는것은 성능향상에 도움이 될 수 없음!__
    - 위 실험결과를 볼 때, 각 네트워크들은 해당 네트워크 구조가 갖는 optimal한 커널 크기 point가 존재함
- 따라서 __정확도와 효율의 향상을 위해선 high-resolution 패턴을 학습할 수 있게 해주는 큰 크기의 커널과 low-resolution 패턴을 학습 할 수 있게 해주는 작은 크기의 커널 모두 필요__

## Related work
