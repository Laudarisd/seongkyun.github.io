---
layout: post
title: FitNets- Hints for Thin Deep Nets
category: papers
tags: [Deep learning]
comments: true
---

# FitNets- Hints for Thin Deep Nets

Original paper: https://arxiv.org/abs/1412.6550

Authors: Adriana Romero, Nicolas Ballas, Samira Ebrahimi Kahou, Antoine Chassang, Carlo Gatta, Yoshua Bengio

## Abstract
- Depth 네트워크는 성능이 좋지만 gradient 방식의 training은 깊은 네트워크의 커진 non-linearity로 인해 학습이 더 어려워졌다. 최근 제안된 knowledge distillation 접근방식은 작고 빠른 모델을 얻기 위하여 큰 네트워크나 앙상블의 soft output을 이용하여 그것을 닮도록 학습하는 방식이다. 본 논문에선 이러한 아이디어를 확장시켜서 깊은 teacher의 중간출력을 이용하여 얕은 student가 더 좋은 성능을 보이도록 학습시키는 방법을 제안한다. Student의 중간 hidden layer가 보통 teacher의 중간 layer보다 작으므로 추가적인 parameter가 teacher hidden layer의 prediction을 student hidden layer가 맞추도록 하는 추가 파라미터가 제안된다. 이로 인해 더 빠르게 동작하거나 더 나은 일반화 성능을 보이는 deeper student net에 대해 capacity를 조정 할 수 있게된다. 예를 들어 CIFAR-10 데이터셋에 대해 deep student network의 경우 10.4배의 적은 파라미터를 이용하여 크고 SOTA 성능을 보이는 teacher network의 성능을 능가 할 수 있게 된다.

## Conclusion
- 논문에서는 student의 training process를 guide하기 위해 teacher의 hidden layer에서 intermediate-level hint를 이용하여 wide하고 deep한 네트워크를 thin하고 deeper한 네트워크로 압축하는 새로운 학습방법을 제안했다. 논문에선 이러한 hint를 사용하여 보다 적은 parameter로 very deep student model을 학습 시킬 수 있었으며, 이 student 모델은 teacher보다 더 나은 일반화 성능을 보이고 더 빠르게 동작하였다. 논문에선 teacher net의 hidden state로 thin and deep network의 inner layer들에게 hint를 주는것이 classification target으로 네트워크를 학습시키는것보다 더 나은 일반화 성능을 보인다는것을 실험적으로 증명했다. 벤치마크 데이터셋에 대한 실험은 capacity가 작은 깊은 네트워크가 10배 이상의 parameter를 가진 네트워크보다 비슷하거나 훨씬 뛰어난 feature 추출 능력이 있음을 보여준다. The hint-based training suggests that more efforts should be devoted to explore new training strategies to leverage the power of deep networks.

## 논문 소개
- 

<center>
<figure>
<img src="/assets/post_img/papers/2019-04-07-fitnets/fig1.jpg" alt="views">
<figcaption></figcaption>
</figure>
</center>
