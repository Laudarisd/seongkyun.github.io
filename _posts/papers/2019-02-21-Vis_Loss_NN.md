---
layout: post
title: Visualizing the Loss Landscape of Neural Nets
category: papers
tags: [Deep learning, Mobilenetv2, Linear bottleneck]
comments: true
---

# Visualizing the Loss Landscape of Neural Nets

Original paper: https://arxiv.org/abs/1712.09913

Authors: Hao Li, Zheng Xu, Gavin Taylor, Christoph Studer, Tom Goldstein

## Abstract

## Introduction

### Contributions

## Theoretical Background

## The Basics of Loss Function Visualization

### 1-Dimensional Linear Interpolation

### Contour Plots & Random Directions

## Proposed Visualization: Filter-Wise Normalization

## The Sharp vs Flat Dilemma

### Filter Normalizaed Plots

## What Makes Neural Networks Trainable? Insights on the (Non)Convexity Structure of Loss Surfaces

### Experimental Setup

### The Effect of Network Depth

### Shortcut Connections to the Rescue

### Wide Models vs Thin Models

### Implications for Network Initialization

### Landscape Geometry Affects Generalization

### A note of caution: Are we really seeing convexity?

## Visualizing Optimization Paths

### Why Random Directions Fail: Low-Dimensional Optimization Trajectories

### Effective Trajectory Plotting using PCA Directions

## Conclusion

<center>
<figure>
<img src="/assets/post_img/papers/2019-02-21-Vis_Loss_NN/fig1.png" alt="views">
<figcaption>contents</figcaption>
</figure>
</center>
