---
layout: post
title: Searching for MobileNetV3
category: papers
tags: [Deep learning]
comments: true
---

# Searching for MobileNetV3

Original paper: https://arxiv.org/pdf/1905.02244.pdf

Authors: Andrew Howard, Mark Sandler, Grace Chu, Liang-Chieh Chen, Bo Chen, Mingxing Tan, Weijun Wang, Yukun Zhu, Ruoming Pang, Vijay Vasudevan, Quoc V. Le, Hartwig Adam (Google)

## Abstract
- 새로운 아키텍쳐를 적용시키는 등의 여러 상호 보완적인 기술들을 조합해서 MobileNet v3를 제안
- MobileNet v3는 mobile phone CPU에 최적화되어있음
  - NetAdapt와 NAS를 조합해서 새로운 구조를 제안
- 본 논문에서는 automated search 알고리즘과 네트워크 디자인이 어떻게 같이 동작해서 전체적인 SOTA 성능을 달성하는지에 대한 보완적인 방법들을 다룸
- 그 과정에서 MobileNetV3-Large와 MobileNetV3-Small이라는 모델을 제안함
  - Large는 high resource 사용시, Small은 low resource 사용시
- 위 두 모델들은 Object detection과 Semantic segmentation에 적용되어 테스트 됨
- Semantic segmentation(or any dense pixel prediction)에서는 효율적인 새로운 segmentation decoder인 Lite Reduced Atrous Spatial Pyramid Pooling (LR-ASPP)를 제안함
- Mobile환경에서의 classification, detection, segmentation task에서 SOTA 성능을 달성했음
- MobileNetV3-Large는 MobileNetV2에 비해 ImageNet classification에서 3.2% 정확하면서도 20%의 latency가 개선됨
- MobileNetV3-Small은 MobileNetV2에 비해 비슷한 latency로 6.6% 더 정확했음
- MobileNetV3-Large는 MobileNetV2에 비해 MS COCO detection에서 25% 빠르면서도 비슷한 정확도를 보였음
- MobileNetV3-Large LR-ASPP는 MobileNetV2 R-ASPP에 비해 Cityspace segmentation에서 34% 빠르면서도 비슷한 정확도를 보였음

## 1. Introduction
- 효율적인 on-device 인공신경망은 mobile 적용 시대에 있어서 매우 흔함
  - 이러한 on-device 딥러닝은 사용자의 개인정보를 서버로 전송하지 않고도 사용자에 최적화된 구동을 위해 필수로 필요한 분야
  - On-device를 가능하게 하는 효율적인 구조들은 높은 정확도와 적은 latency와 함께 효율적인 구동으로 인해 mobile device의 battery life를 늘려줌
- 본 논문에선 on-device computer vision을 강화하기 위해 더 정확하고 효율적인 MobiletNetV3 Large와 Small 모델들을 제안함
- 논문에서 제안하는 네트워크는 SOTA 성능을 뛰어넘었으며 automated search와 새로운 아키텍처에 결합해 효과적으로 새로운 모델을 구축하는 방법을 설명
- 논문의 목표는 accuracy-latency 최적화를 통해 mobile환경에서 최고의 mobile computer vision architecture를 제안하는 것
- 이를 위해 다음의 것들을 설명
  1. Complementary search techniques
  2. Mobile setting에 효율적인 새로운 non-linearities practical version을 제안
  3. 새로운 효율적인 네트워크 디자인
  4. 새로운 효율적인 segmentation decoder
- 위의 것들을 mobile phone에서 다양하고 광범위한 방법으로 효율성등을 실험적으로 검증함
- 아래의 흐름을 따름
  - Section 2에선 related work에 대해서 다룸
  - Section 3에선 mobile model들의 efficient building block들에서 사용된 방법들을 리뷰
  - Section 4에선 NAS와 MnasNet, NetAdapt 알고리즘들의 상호적인 보완적 특성을 다룸
  - Section 5에선 joint search를 통해 찾아진 모델의 효율을 높히는 새로운 architecture design을 설명
  - Section 6에선 classification, detection, segmentation task를 이용해 모델의 효율과 각 적용요소들의 contribution에 대해 실험하고 결과를 설명
  - Section 7에선 결론 및 future work를 다룸

## 2. Related Work
- 최근 다방면에서 뉴럴넷의 최적의 정확도-효율 trade-off를 찾기위한 다양한 연구들이 수행됨
  - Hand-crafted 구조들과 NAS를 이용해 찾아진 구조들 모두 이 분야의 연구를 위해 주요하게 사용됨

- SqueezeNet[22]은 squeeze와 expand 모듈과 1x1 컨벌루션을 광범위하게 사용해 파라미터 수를 줄이는것에 중점을 두고 연구되었음
  - 최근에는 파라미터 수를 줄이는 것 뿐만 아니라 실질적인 latency를 줄이기 위해 연산량(MAdds)을 줄이기 위한 연구가 수행됨
- MobileNetV1[19]은 연산 효율 증가를 위해 depthwise separable convolution을 사용함
- MobileNetV2[39]은 위의 방법을 이용하면서도 resource-efficient한 inverted residual block과 linear bottleneck을 제안함
- ShuffleNet[49]은 group convolution과 channel shuffle 연산을 활용해 연산량을 줄임
- CondenseNet[21]은 모델 학습단에서 group convolution을 학습시켜 feature 재사용을 위한 layer간 dense connection을 활용했음
- ShiftNet[46]은 연산비용이 비싼 spatial convolution을 대체하기 위해 point-wise convolution을 중간에 끼워넣은 shift operation을 제안함

- 강화학습을 이용한 NAS로 찾아진 효율적이면서도 competitive한 정확도를 갖는 architecture design들이 있음[53, 54, 3, 27, 35]
- A fully configurable search space can grow exponentially large and intractable.
- 따라서 초기 NAS 연구들은 cell level structure에 집중되었으며, 이로인해 같은 cell들이 모든 layer들에서 재사용되는 구조를 가졌음
- 최근 [43]과 같은 연구에선 block-level의 계층적인 search space에 대해 연구하며 다른 layer structure를 다른 resolution block에서 사용 가능하게 했음
- 네트워크 탐색 과정의 연산비용 감소를 위해서 [28, 5, 45]등에선 gradient based optimization이 적용된 differentiable architecture search framework가 사용되었음
- 또한, 현존하는 네트워크를 강제로 mobile platform에 최적화 시키기 위해 [48, 15, 12]에서는 더 효율적인 automated network simplification algorithm들을 제안함

- [23, 25, 47, 41, 51, 52, 37]에선 양자화(quantization)라는 또다른 complementary effort를 적용시켜 precision arithmetic을 줄여서 네트워크를 효율화시킴
- 마지막으로 [4, 17]에서는 지식증류(knowledge distillation)를 이용해 추가적인 complementary method를 제안했으며, 크고 정확한 teacher network를 통해 작고 부정확한 student network의 효율이 향상됨

## 3. Efficient Mobile Building Blocks

<center>
<figure>
<img src="/assets/post_img/papers/2019-12-03-mbv3/fig1.PNG" alt="views">
<figcaption></figcaption>
</figure>
</center>

