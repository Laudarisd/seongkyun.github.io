---
layout: post
title: SSD- Single Shot Multibox Detector
category: papers
tags: [Deep learning]
comments: true
---

# SSD: Single Shot Multibox Detector

Original paper: https://arxiv.org/pdf/1512.02325.pdf

Authors: Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, Cheng-Yang Fu, Alexander C. Berg

Implementation code: https://github.com/amdegroot/ssd.pytorch (Pytorch)

- 대략적인 SSD의 프로세스를 알지만 구체적으로 정확하고 자세하게 분석되어있지 않아 직관적 이해를 위해 다룸
  - 참고 글: https://taeu.github.io/paper/deeplearning-paper-ssd/

## Abstract
- SSD discretizes the output space of bounding boxes into a set of default boxes over different aspect ratios and scales per feature map location(multiple feature map).
- 즉, 아웃풋을 만드는 multi-feature map을 나눈 다음 각 feature map에서 다른 비율과 스케일로 default box를 생성한 후, 모델을 통해 계산된 좌표와 클래스값에 default box를 활용해 최종 bounding box를 생성한다.

## Introduction
- 섹션 2.1 Model과 2.2 Training에서 box의 class 점수와 위치좌표 크기를 예측하는데 고정된 default box를 예측하도록 하는 내용을 다룸
- 정확도 향상을 위해 서로 다른 피쳐맵에서 서로 다른 스케일의 예측을 할 수 있게 함.
  - YOLO v1의 경우 최종 아웃풋으로 나온 하나의 feature map의 각 그리드 셀당 2개의 bounding box를 예측하도록 했음
  - SSD는 이에 반해 여러가지의 grid cell을 갖고 각 feature map당 여러가지(보통 6개) 바운딩박스를 가짐
  - 2.1과 2.2에서 상세설명

## The Single Shot Detector (SSD)
### 2.1 Model
- Image detection의 목적상 들어온 영상에서 객체의 위치와 크기, 레이블을 찾아야 함.
- 따라서 input으로는 이미지, output으로는 class score, x, y, w, h가 됨.

<center>
<figure>
<img src="/assets/post_img/papers/2019-07-01-SSD/fig1.jpg" alt="views">
<figcaption></figcaption>
</figure>
</center>

- 논문의 SSD 구조는 위와 같음(VGG16 backbone)

<center>
<figure>
<img src="/assets/post_img/papers/2019-07-01-SSD/fig2.png" alt="views">
<figcaption></figcaption>
</figure>
</center>

- 우선 SSD는 저해상도에서도 작동이 잘 되기에 300x300 pixel image를 기본적으로 입력받도록 함.
- Input image를 기본적으로 처리할땐 backbone인 VGG16을 갖고와 conv4_3까지만 가져다가 씀
  - 300x300x3 input이 backbone 통과 후 38x38x512가 됨

<center>
<figure>
<img src="/assets/post_img/papers/2019-07-01-SSD/fig3.png" alt="views">
<figcaption></figcaption>
</figure>
</center>

- 다음으로 논문에서 강조하는 multi feature maps에 해당하는 부분으로, 각각 위의 사진의 크기를 갖는 feature map들을 backbone과 extra network가 포함된 feature extractor에서 가져와서 그 multi-feature map들을 이용하여 detection을 수행하게 됨.

<center>
<figure>
<img src="/assets/post_img/papers/2019-07-01-SSD/fig4.png" alt="views">
<figcaption></figcaption>
</figure>
</center>

- 각 feature map에서적절한conv 연산을 이용해 우리가 예측하고자 하는 bounding box의 정보들(x, y, w, h, class scores)을 예측함.
- 여기서 conv filter size는 3 x 3 x (# of bounding box x (class score + offset)) 이 됨
  - stride = 1, padding = 1로 추정
- 이 6개의 서로 다른 크기를 갖는 feature map들 각각에서 예측된 bounding box의 개수 합은 하나의 클래스당 8732개가 됨

<center>
<figure>
<img src="/assets/post_img/papers/2019-07-01-SSD/fig5.png" alt="views">
<figcaption></figcaption>
</figure>
</center>

- 8732개의 bounding box의 output이 나온다고 해서 그것을 다 고려하지 않음
  - Default box간의 IOU를 계산한 후 0.5가 넘는 box들만 출력결과에 포함시키고 나머지는 0으로 하여 실효성 없는 데이터를 삭제함
  - 이 box들을 NMS를 거쳐서 중복되는 box를 제거
  
<center>
<figure>
<img src="/assets/post_img/papers/2019-07-01-SSD/fig6.png" alt="views">
<figcaption></figcaption>
</figure>
</center>

- 마지막으로 NMS를 통해 최종 detection 결과는 위 그림에서 우측 상단과 같음

#### Multi-scale feature maps for detection
- 38x38, 19x19, 10x10, 5x5, 3x3, 1x1 의 다양한 크기를 갖는 피쳐맵들을 의미
- Yolo는 7x7 grid 만을 이용했지만, SSD는 전체 이미지를 38x38, 19x19, 10x10, 5x5, 3x3, 1x1의 그리드로 나누고 이를 predictor layer와 연결하여 결과를 추론
- 큰 피쳐맵에서는 작은 물체 탐지, 작은 피쳐맵에서는 큰 물체 탐지 (뒤의 2.2 training 부분에서 더 자세히 다룸)

#### Convolutional predictors for detection
- 위에서 생성된 feature map은 3x3 kernel size stride=2 conv layer와 연결
- Feature map은 3x3xp size의 필터로 conv 연산.
  - Yolo v1은 fc layer를 사용하여 x, y, w, h, score를 추론
- 예측된 결과는 x, y, w, h, score(offsets)를 의미

#### Default boxes and aspect ratios
- 각 feature map에 grid cell을 만들고(5x5와 같이..) default bounding box를 만들어 그 default box와 대응되는 자리에서 예측되는 박스의 offset과 per-class scores(여기서는 박스 안에 객체가 있는지 없는지를 예측)를 예측
- 이 때 per-class scores를 클래스 확률로 생각하면 안되고, 해당 박스 안에 객체가 있는지 없는지를 나타내는 값이라고 생각해야 하며 자세한것은 뒤쪽의 matching strategy에서 설명
- 6개의 feature map(마지막 prediction layer와 연결된 feature map)은 각각 연산을 통해 conv(3\*3\*(#bbx\*(c+offset))) 출력을 생성
- Output은 각 셀당 #bb 개의 bounding box를 예측

### 2.2 Training

<center>
<figure>
<img src="/assets/post_img/papers/2019-07-01-SSD/fig1.png" alt="views">
<figcaption></figcaption>
</figure>
</center>
