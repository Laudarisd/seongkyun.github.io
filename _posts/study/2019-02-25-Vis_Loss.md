---
layout: post
title: Neural network loss landscape 시각화하기
category: study
tags: [loss landscape, visualization]
comments: true
---

# Neural network loss landscape 시각화하기(Visualizing the Loss Landscape of Neural Nets)
- Visualizing the Loss Landscape of Neural Nets 논문에 대한 implementation code다.
- Codes from https://github.com/tomgoldstein/loss-landscape
- `git clone https://github.com/tomgoldstein/loss-landscape.git` 로 코드 다운로드

## 환경 설정
- 하나 이상의 GPU
- Pytorch 0.4
  - virtual env를 만들어 별도로 설치
  - pytorch 0.4, python 3.6, cuda 9.0
  - `pip install https://download.pytorch.org/whl/cu90/torch-0.4.0-cp36-cp36m-linux_x86_64.whl`
- openmpi 3.1.2
  - https://www.open-mpi.org/software/ompi/v3.1/ 에서 3.1.2 탭의 `openmpi-3.1.2.tar.gz` 다운로드
  - `mkdir ~/openmpi` 폴더 만들고 해당 폴더로 `openmpi-3.1.2.tar.gz`파일 이동 후 `tar -zxvf openmpi-3.1.2.tar.gz`로 압축 풀기
  - `cd openmpi-3.1.2/` 로 폴더 이동
  - `./configure --prefix=$HOME/openmpi`로 configure 후 compile
  - `make -j8 all`로 make(5~10분 소요)
  - `make install`로 설치
  - `vim ~/.bashrc`로 bashrc에 path 설정
    - `export PATH=$HOME/openmpi/bin:$PATH`
    - `export LD_LIBRARY_PATH=$HOME/openmpi/lib:$LD_LIBRARY_PATH`
  - 저장 후 나가서 `source ~/.bashrc`
  - `mpiexec --version`과 `mpirun --version`으로 설치 확인(버전 뜨면 설치 잘 된 상태)
- mpi4py 2.0.0
  - `pip install mpi4py==2.0.0`
- numpy 1.15.1
  - `pip install numpy`
- h5py 2.7.0
  - `pip install h5py==2.7.0`
- matplotlib 2.0.2
  - `pip install matplotlib`
- scipy 0.19
  - `pip install scipy`
- seaborn
  - `pip install seaborn`
- scikit-learn
  - `pip install scikit-learn`

## Pre-trained models
- https://github.com/tomgoldstein/loss-landscape 에서 pretrained weight file들을 받아 해당 directory 내의 폴더에 저장
  - ex. `..../loss-landscape/cifar10/trained_nets/`

## Visualizing 1D loss curve
- 실제 코드 돌릴때에는 `openmpi` 사용시 코드가 돌지 않아 그냥 돌렸다.
  - 하기 코드에서 `mpirun -n 4`를 빼고 돌리면 됨

### Creating 1D linear interpolations
- 1D linear interpolation 방법[1]은 같은 network loss function의 두 개의 minimizer사이의 방향을 따라 loss 값을 평가한다.
- 이 방법은 다양한 batch size로 학습된 네트워크의 minizer의 평탄도를 비교하기 위해 적용된다[2].
- 1D linear interpolation plot은 plot_surface.py를 사용하여 생성된다.

```
{%raw%}
mpirun -n 4 python plot_surface.py --mpi --cuda --model vgg9 --x=-0.5:1.5:401 --dir_type states \
--model_file cifar10/trained_nets/vgg9_sgd_lr=0.1_bs=128_wd=0.0_save_epoch=1/model_300.t7 \
--model_file2 cifar10/trained_nets/vgg9_sgd_lr=0.1_bs=8192_wd=0.0_save_epoch=1/model_300.t7 --plot
{%endraw%}
```

- `--x=-0.5:1.5:401`: Plot의 범위(range)와 해상도(resolution)를 설정한다. Plot의 x축 좌표는 -0.5부터 1.5까지(minimizer들이 각각 0과 1에 위치하므로)로 설정되며, loss값들은 x축을 따라 총 401개의 위치에서 측정된다.
- `--dir_type states`: BN layer들의 통계(running_mean과 running_var) 뿐만 아니라 모든 parameter들의 dimension을 포함하는 방향을 의미한다. 하나의 화면에서 두 개의 솔루션을 한번에 plot할 때 running_mean과 running_var를 무시하게 될 경우 올바른 loss값을 생성 할 수 없다.

### Producing plots along random normalized directions
- 모델에서 생성된 parameter들과 동일한 차원의 random direction을 만들고 filter-normalize된다. 다음으로 loss값을 그 방향에 따라 sample할 수 있게된다.

```
{%raw%}
mpirun -n 4 python plot_surface.py --mpi --cuda --model vgg9 --x=-1:1:51 \
--model_file cifar10/trained_nets/vgg9_sgd_lr=0.1_bs=128_wd=0.0_save_epoch=1/model_300.t7 \
--dir_type weights --xnorm filter --xignore biasbn --plot
{%endraw%}
```

- `--dir_type weights`: Direction이 BN 레이어의 bias 및 parameter들을 포함하며, 학습 된 parameter와 동일한 크기를 가진다.
- `-- xnorm filter`: Filter level에서의 random direction을 normalize한다. 여기서 filter는 single feature map을 생성하는 parameter들을 의미한다. Fully connected layer의 경우 filter는 단일 뉴런에 기여하는 weight를 포함한다.
- `--xignore biasbn`: Bias 및 BN parameters에 해당하는 direction을 무시한다.(Random vector의 해당 항목을 0으로 채움)

## Visualizing 2D loss contours

```
{%raw%}
mpirun -n 4 python plot_surface.py --mpi --cuda --model resnet56 --x=-1:1:51 --y=-1:1:51 \
--model_file cifar10/trained_nets/resnet56_sgd_lr=0.1_bs=128_wd=0.0005/model_300.t7 \
--dir_type weights --xnorm filter --xignore biasbn --ynorm filter --yignore biasbn  --plot
{%endraw%}
```

## Visualizing 3D loss surface

```
{%raw%}

{%endraw%}
```

---

__References__

- [1] Ian J Goodfellow, Oriol Vinyals, and Andrew M Saxe. Qualitatively characterizing neural network optimization problems. ICLR, 2015.
- [2] Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Peter Tang. On large-batch training for deep learning: Generalization gap and sharp minima. ICLR, 2017.
