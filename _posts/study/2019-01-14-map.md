---
layout: post
title: Mean Average Precision(mAP) on Object detection
category: study
tags: [mean average precision, map, object detection]
comments: true
---

# Mean Average Precision(mAP) on Object detection

Object detection에서는 모델의 성능(정확도)을 주로 Mean average precision(mAP)를 통해 확인한다. mAP가 높을수록 정확하고, 작을수록 부정확하다.
그런데 보통 모델의 mAP의 측정 해당 데이터셋별(PASCAL VOC, MS COCO 등등..)로 잘 짜여진 코드가 많이 있어서 알고리즘적으로 어떻게 계산되는지 자세히는 알지 못했다.

이번 글에서는 mAP에 대해 보다 자세히 알아보기 위한 공부를 한 내용을 정리했다.

- 기본적으로 mAP는 다른 recall 값에서의 평균 최대 precision으로 정의

## Precision, recall and IoU

### Precision
- Precision은 모델의 출력이 얼마나 정확한지를 측정하는 것.
- 즉, 모델이 예측한 결과의 Positive 결과가 얼마나 정확한지를 나타내는 값.

### Recall
- Recall은 모델의 출력이 얼마나 Positive 값들을 잘 찾는지를 측정하는 것.

### Precision과 Recall의 수학적 정의

$$TP=True \; positive \\ TN=True \; negative \\ FP=False \; positive \\ FN=False \; negative$$

$$Precision=\frac{TP}{TP+FP}$$

$$Recall=\frac{TP}{TP+FN}$$

$$F1=2\cdot \frac{precision\cdot recall}{precision+recall}$$

- 예를 들어, 암 진단의 경우에 대해 Precision과 Recall은 다음과 같이 정의 됨.
  - $TP$: 실제 암 세포들의 개수
  - $total \; positive \; results$: 암세포라고 판단 된 결과
  - $total \; cancer \; cases$: 전체 암 세포들의 개수

$$Precision=\frac{TP}{total \; positive \; results}$$

$$Recall=\frac{TP}{total \; cancer \; cases}$$

### IoU (Intersection over union)
- 2개 영역 사이의 중첩되는 정도를 측정.
- 이는 Object detector가 실제 Ground truth와 예측 결과(Prediction)가 얼마나 정확히 겹치는지를 계산하여 예측이 얼마나 잘 되는지를 측정.

<center>
<figure>
<img src="/assets/post_img/study/2019-01-14-map/fig1.png" alt="views">
<figcaption>IoU의 정의</figcaption>
</figure>
</center>

## Average precision (AP)
- AP를 계산하는 간단한 예제를 살펴보자.
  - 데이터셋에는 총 5개의 사과가 있음
  - 모델이 사과에 대해 예측한 모든 결과를 수집하고 예측 신뢰 수준(predicted confidence level)에 따라 순위를 지정하여 나열
  - 두 번째 열은 예측 결과가 올바른지 여부를 나타냄
  - 예측이 정확하다고 판단(Correct=True)하는 기준은 Ground Truth(GT)와의 IoU가 50%가 넘는 경우($IoU\geq 0.5$)

<center>
<figure>
<img src="/assets/post_img/study/2019-01-14-map/fig2.png" alt="views">
</figure>
</center>



- [참고 글]

https://medium.com/@jonathan_hui/map-mean-average-precision-for-object-detection-45c121a31173
