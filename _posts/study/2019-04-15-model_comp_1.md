---
layout: post
title: Model compression(Pruning/Distillation) 정리
category: study
tags: [Model compression, Pruning, Distillation]
comments: true
---

# Model compression(Pruning/Distillation) 정리

## Model Compression
- __Model compression?__ 큰 모델을 작게 만드는 방법
  - 모델의 효율을 좋게 하도록 redundancy를 줄이거나 efficiency를 높이는 방식
- Pruning / Distillation
  - Pruning: 모델에서 중요도가 낮은 뉴런을 제거하여 직접적인 모델 크기의 감소
    - 모델의 redundancy를 줄이는 방법
    - 장점: 모델 최적화에 따른 연산량 감소 및 모델사이즈 축소율이 상당함
    - 단점: 모델 학습 및 pruning 과정이 복잡하고 매우 오래 걸리며, task별로 다른 적용방법을 필요로함

  - Distillation: 작은 모델에 큰 모델의 정보(knowledge)를 전달하여 작은 모델의 정확도를 향상시킴
    - 모델의 efficiency를 높이는 방식
    - 장점: Pre-trained model을 이용해 빠르게 학습시킬 수 있으며 범용적으로 적용 가능
    - 단점: 모델 최적화 개념보다는 존재하는 모델의 표현력(capacity)을 최대한 사용하게 되어 정확도가 향상되어 큰 모델을 대체할 수 있게 되는 간접적인 모델 사이즈 축소방법

## Network Pruning
- 네트워크의 수많은 파라미터중 중요하지 않은 파라미터들을 제거하는 방법
  - 어떠한 기준을 정하여 rank를 만들도록 뉴런을 정렬(ranking)
  - Ranking에 따라 일정 threshold를 넘지 못하는 뉴런을 제거
    - 이 과정에서 뒤에 연결된 뉴런에까지 모두 영향을 미치므로 파라미터가 많이 제거됨(pruning)
    - Pruned 네트워크의 정확도는 전에비해 조금 감소됨
    - 너무 많이 pruning이 되면 네트워크가 손상되어 정확도 회복이 불가능
  - Pruning 후 네트워크를 재학습시켜 정확도를 회복
    - Interactive Pruning

<center>
<figure>
<img src="/assets/post_img/study/2019-04-13-pruning/fig1.png" alt="views">
<figcaption>Interactive pruning</figcaption>
</figure>
</center>

- Pruning Convolutional Neural Networks for Resource Efficient Inference
  - ICLR 2017 논문(Nvidia)
    - Pre-trained VGG16의 filter를 random하게 골라 pruning하고 validation set에서 cost function의 변화를 제일 적게 하는 뉴런의 rank가 높도록 하여 나열
    - 실험결과 VGG16 네트워크 모델사이즈 10배정도 줄일 수 있었음
      - Caltech Birds-200 dataset 이용

## Network Pruning on SSD
- Multi-layer Pruning Framework for Compressing Single Shot MultiBox Detector
  - Singh, Pravendra, et al. 2019 IEEE Winter Conference on Applications of Computer Vision (WACV). IEEE, 2019.
  - Single Shot Multi-Box Detector(SSD)를 pruning
    - Object detection: PASCAL VOC dataset에 대해 정확도는 유지하며 6.7~4.9배의 모델 압축(VGG16 based)
    - Classification: CIFAR dataset에 대해 정확도는 유지하며 125배의 모델 압축(VGG16)
  1. Sparsity induction: Pre-trained SSD 를 L1-norm이 적용된 loss function으로 threshold 이하의 weight를 0으로 만들어 pruning 될 layer 집합 𝐿을 만듦
    - L1-norm에는 test set과 val set을 이용함
    - 기준이 될 threshold는 validation set을 이용하여 해당 레이어의 값 평균을 기준으로 정함
  2. Filter selection: 이렇게 선택된 layer 집합 𝐿의 레이어 𝑙의 중요도를 평가
    - 레이어 𝑙과 다음 레이어 𝑙+1간의 filter sparsity statics를 이용하여 중요도 평가
    - 이 과정에서 레이어 l에서 중요하지 않다고 판단되는 filter의 list를 얻음
    - Layer 집합 𝐿에 대해 모두 반복수행
  3. Pruning: 앞에서 중요하지 않다 판단된 레이어 𝑙과 해당 output과 연결된 layer 𝑙+1을 제거
    - Layer 집합 𝐿에 대해 모두 반복수행
  4. Retraining: 앞에서 pruning된 네트워크를 original loss(SSD)를 이용해 떨어진 정확도를 복원(재학습)



