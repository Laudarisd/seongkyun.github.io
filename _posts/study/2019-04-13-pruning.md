---
layout: post
title: Pruning deep neural networks to make them fast and small
category: study
tags: [Cosine distance]
comments: true
---

# Pruning deep neural networks to make them fast and small
- 참고글: https://jacobgil.github.io/deeplearning/pruning-deep-learning

- Pytorch implementation code: https://github.com/jacobgil/pytorch-pruning
- paper: https://arxiv.org/pdf/1611.06440.pdf

## Network pruning
- Neural network의 pruning은 1990년대에 제안된 아이디어다. 기본 아이디어는 네트워크의 많은 파라미터중 어떤것들은 중요도나 필요성이 떨어지는(redundant) 것들이 있으며, 따라서 output의 출력에 큰 영향력을 끼치지 못하는 파라미터들이 존재한다는 것이다.
- 만약 네트워크의 뉴런들을 어떤 뉴런이 더 중요한지에 따라 순서를 매겨서 나열 할 수 있다면, 낮은 순위에 존재하는 뉴런들은 상대적으로 output의 출력에 대한 기여도가 떨어지기 때문에 네트워크에서 그 뉴런들을 제거 할 수 있게 되며, 이로 인해 네트워크가 더 빠르게 동작 할 수 있게 된다.
- 이러한 빠르고 작은 네트워크는 mobile device에서 빠른 동작을 위한 필수요소다.
- 중요도를 정하는것(ranking)은 뉴런 weight의 L1/L2 평균, mean activations, 뉴런의 출력이 어떠한 validation set의 입력이 들어왔을 때 0이 아닌 횟수 등 몇몇의 다양한 방법들에 의해 정렬 될 수 있다. 이러한 pruning 다음엔 정확도가 떨어지게 되지만(하지만 ranking이 제대로 되었다면 조금밖에 떨어지지 않음) 보통은 네트워크를 더 학습시켜 이를 복구시킬 수 있게된다. 만약 한번에 너무 많이 pruning이 될 경우엔 네트워크가 손상되어 정확도의 복구가 불가능하게 된다. 따라서 실제로는 pruning이 interactive process가 되어야 한다. 
  - 이를 "Interactive Pruning" 이라 하며, Prune-Train-Repeat으로 수행된다.

<center>
<figure>
<img src="/assets/post_img/study/2019-04-13-pruning/fig1.png" alt="views">
<figcaption>Image from 'PRUNING CONVOLUTIONAL NEURAL NETWORKS FOR RESOURCE EFFICIENT INFERENCE'</figcaption>
</figure>
</center>

## Pruning이 popular하지 않은 이유는?
- 실제로 pruning에 대한 수많은 논문들이 존재하지만 real life상에서 접할 수 있는 딥러닝 프로젝트에 적용된경우는 드물다. 그 이유는 다음과같이 추측이 가능하다.
  - Ranking하는 방법이 아직까진 그렇게 좋지 못하므로 정확도의 향상이 크다.
  - 실제 적용에 매우 힘들다.
  - Pruning을 실제 사용한 경우라도 official하게 내놓지 않는다(secret sauce advantage).
- 이러한 이유들로 인해 implementation을 한다음 그 결과를 확인해본다.
- 본 글에선 몇몇의 pruning 방법들에대해 살펴보고 최근논문에서 제안하는 방법을 implementation한다.
- Kaggle Dogs vs Cats dataset에서 개와 고양이 분류를 위한 VGG network를 fine-tuning한다.(tranfer learning)
- 네트워크 pruning 뒤엔 속도가 3배, 크기가 4배가량 감소되었다.

## Pruning for speed vs Pruning for a small model
- VGG16 weight의 90%가량은 fully connected(fc) layer이지만 전체 floating point operation의 1%만을 차지한다.
- 몇몇 연구들은 fc layer 수를 줄이는데 중점을 두었으며, fc layer들을 pruning하여 모델 사이즈를 크게 줄일 수 있었다.
- 하지만 본 글에선 convolutional layer에서 전체 filter를 pruning하는데 중점을 둔다.
- 또한 pruning을 통해 네트워크 사이즈를 줄임으로써 사용될 메모리가줄어들게 된다. [1611.06440 Pruning Convolutional Neural Networks for Resource Efficient Inference] 논문에서 레이어가 깊어질수록 pruning에 의한 이점은 더 커지게 된다.
- 이는 마지막 convolutional layer가 많이 pruning 될수록 그 뒤에 연결되어있는 fc layer도 같이 정리가 되게 되므로 많은 메모리 절약의 효과를 가져오게 되기 때문이다.
- Convolutional filter를 pruning할 때의 다른 option으로 각 필터의 weight를 줄이거나 단일 kernel에서의 특정한 dimension을 제거하는 방법이 있다. 하지만 몇몇의 filter에 대해서만 듬성듬성하게 pruning을 적용하여 계산 속도를 높히는것은 불가능하므로 최근의 연구들은 전체 filter들이 pruning하는 대신에 "Structured sparsity"를 선호한다.
- 이러한 논문들에서 다뤄지는 것들중 제일 중요한 것으로 큰 네트워크를 학습시킨 다음에 pruning함에 있어서, 특히 transfer learning의 경우, 작은 네트워크를 처음부터 학습시키는것보다 훨씬 더 나은 결과를 얻을 수 있다는 점이다.
- 아래에서 몇 가지 방법들에 대해 다뤄본다.









