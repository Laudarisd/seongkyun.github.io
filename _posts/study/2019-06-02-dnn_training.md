---
layout: post
title: 인공신경망 학습 레시피
category: study
tags: [Convolutional Neural Network, Training]
comments: true
---

# 인공신경망 학습 레시피
- 참고 글
  - https://karpathy.github.io/2019/04/25/recipe/
  - https://medium.com/@bntejn/%EC%9D%B8%EA%B3%B5%EC%8B%A0%EA%B2%BD%EB%A7%9D-%ED%95%99%EC%8A%B5-%EB%A0%88%EC%8B%9C%ED%94%BC-%EB%B2%88%EC%97%AD-70c5e58341ec
    - 원글의 번역본을 참고

- 네트워크의 학습시 시 발생할 수 있는 오류들을 줄이기 위한 프로세스를 정리
- 본문 시작전에 다음의 두 가지 중요 관찰사항에 대해 논함

## 1. 인공신경망의 학습과정은 딥러닝 라이브러리를 이용하여 완벽히 추상화하는것이 불가능함
- 보통의 딥러닝 라이브러리를 이용한 네트워크의 학습은 아래와 같은 흐름을 따름

```
{%raw%}
dataset = my_dataset
model = my_model(my_transform, dataset, ResNet50, SGDOptimizer)
{%endraw%}
```

- 복잡한 실제 연산의 과정을 딥러닝 라이브러리는 위의 코드처럼 쉽게 표현되어있음
  - 하지만 실제로는 그렇지 않음
- 일반적인 딥러닝 모델의 학습에 역전파(back-propagation)와 SGD만 적용한다고 해서 인공신경망이 자동으로 동작하지 않음
  - Batch-norm을 적용한다고 해서 optimization이 마술처럼 이루어지는것도 아니고
  - RNN을 도입한다 해서 텍스트를 마술처럼 자동으로 이해하게 되는것도 아니며
  - 강화학습으로 문제를 정의할 수 있다 하여 꼭 그렇게 풀어야 하는것도 아님

## 2. 학습의 실패는 예고없이 등장함
- 코딩할 때 코드를 잘못 짜거나 설정을 잘못한다면 종종 예외처리문(error)을 만나게 됨
  - 문자열 자리에 정수를 넣거나, 매개변수 갯수가 다르다거나, import가 실패하거나, 혹은 입출력의 모양이 다르거나..
- 인공신경망 학습에 있어서 위와같은 쉬운 오류 외에 어디서 잘못되었는지도 모르는 오류가 발생할 확률이 매우 큼
  - 오류 자체가 구문적이며 단위적으로 테스트하며 디버깅하기가 매우 까다로움
  - 예를들어, 
    - Data augmentation시 이미지의 좌우를 뒤집으면서 lable도 뒤집는것을 깜빡해서 인공신경망이 이러한 잘못된 정보까지 학습하는 바람에 오히려 모델의 정확도가 향상 될 가능성도 존재하게 됨.
    - 자기회기모델(auto-regressive model)을 학습시키며 off-by-one bug(배열이나 루프에서 바운더리 컨디션 체크와 관련한 논리적 에러)로 인해 예측하려는 값을 input으로 취하는 실수
    - Gradient를 잘라낸다는게 loss를 잘라내서 학습과정에서 outlier data들이 무시되게 될 수도 있음
    - 미리 학습해둔 checkpoint로부터 weight를 초기화했는데 해당 평균값을 쓰지 않는 실수
    - 정규화 세팅, 학습률, 학습률 감소율, 모델 크기 등에 대한 설정 잘못
  - 위의 경우 외에도 어떠한 잘못된 설정으로 네트워크를 학습시켰을 때 정확도가 좋아질 수 있고, 이는 순전히 운이 좋은경우에 해당
  - 대부분의 경우 학습은 문제없다는듯이 잘 진행되며 성능만 살짝 덜어지게 됨

- 이로인해 __신경망을 학습하기 위한 방법으로 빠르고 강력한 방법은 전혀 효과적이지 않고__ 오히려 학습에 어려움을 줄 수 있음
  - 네트워크를 빠르게 학습시키는 여러 방법들이 실제로는 효용성이 떨어진다는 의미로 판단됨.
- 따라서 네트워크의 학습에는 조심스럽고 방어적이고 시각화를 중요하게 여기는 방향으로 접근해야 학습에 좀 더 효과적임
  - 글쓴이의 경험에 따르면.. 참을성 있게 디테일에 집착하는 태도가 딥러닝 모델의 학습을 성공시키는데 가장 중요한 요소라고 함.

## 레시피
- 위에서 언급한 두 사실에 근거하여 저자가 제안하는 늘 참고하는 구체적인 학습의 프로세스를 소개함
  - 각 단계를 거치며 모델은 단순하게 시작해서 점차 복잡해지며, 매 단계마다 어떤 response가 발생할지에 대한 구체적 가설을 세우고, 각 단계가 적용된 후 혹시 발생했을지 모를 문제를 찾아내기 위해 실험과 검증을 반복
- 이렇게 보수적(조심스럽게)으로 접근하는 이유는 검증되지 않은 복잡성(새로운 것을 적용하는것)의 증가를 최대한 방지하고, 발견이 힘들거나 발견자체가 불가능할 수 있는 버그나 오류를 최대한 예방하기 위함임
- 네트워크의 학습시키는 코드를 짜는것을 인공지능망 학습과정에 비유하면 작은 학습률로 학습을 시작한 뒤 매 iteration마다 테스트 데이터셋 전체를 평가하듯이 각 단계를 진행해야 함.
  - 하나의 방법을 적용하고 effectiveness validation 후 다음 방법을 적용해야 함을 의미

### 1. 데이터와 하나가 되기
- __인공신경망 학습의 첫 단계는 코드는 건드리지 않고 학습용 데이터셋을 철저하게 살피는것!(매우 중요)__
  - 수천개의 데이터를 훑어보며 분포를 이해하고 패턴을 찾는 과정
  - 중복된 데이터, 손상된 이미지, 손상된 레이블 등을 발견하고 제거
  - 데이터의 불균형, 편향을 발견하고 조절
- 데이터의 분석을 통해 궁극적으로 사용하게 될 아키텍쳐에 대한 힌트를 얻을 수 있음
  - 예를 들어 아주 지역적인 특성들 만으로 충분한지, 혹은 전역적인 맥락이 필요한지, 얼마나 많은 변화가 있고 어떤 형태들을 갖는지, 어떠한 비정상적인 변화가 감지되고, 전처리를 통해 제거가 가능한지, 공간적 위치가 중요한지, 어떤 pooling 방식이 좋을지, 세부사항이 얼마나 중요하고 얼마나 많은 이미지들을 샘플링을 통해 줄일 수 있을지, 레이블에 얼마나 노이즈가 많은지 등을 살펴볼 수 있음

- 또한 인공신경망이란 결국 데이터를 압축하고 일반화시켜주는 도구이기에 네트워크의 에러를 보고 어디가 잘못된 것인지도 알 수 있음
  - 만약 정확도가 원래의 것보다 떨어진다면 무언가 잘못되었다는걸 직관적으로 알 수 있게 됨

- 데이터의 품질에 대한 대략적인 감을 잡았다면 학습시킬 데이터를 찾고, 걸러내고, 정렬하기 위한 간단한 코드를 작성
  - 그 기준은 레이블의 타입, annotation,의 크기와 숫자 등 고려 가능한 어떠한 것이 될 수 있음
  - 각 기준에 따른 데이터의 분포를 시각화해보고 각 기준에 따라 시각화 했을 때 분포를 벗어나는 튀는 outlier들을 찾아봄
  - Outlier는 대부분 데이터 품질이나 전처리 과정의 오류로 인해 발생했을 가능성이 큼

### 2. 학습에서 평가까지 전 단계를 아우르는 골격을 먼저 짜고 기준 성능을 측정
- 




