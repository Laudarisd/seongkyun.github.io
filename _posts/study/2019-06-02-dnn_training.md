---
layout: post
title: 인공신경망 학습 레시피
category: study
tags: [Convolutional Neural Network, Training]
comments: true
---

# 인공신경망 학습 레시피
- 참고 글
  - https://karpathy.github.io/2019/04/25/recipe/
  - https://medium.com/@bntejn/%EC%9D%B8%EA%B3%B5%EC%8B%A0%EA%B2%BD%EB%A7%9D-%ED%95%99%EC%8A%B5-%EB%A0%88%EC%8B%9C%ED%94%BC-%EB%B2%88%EC%97%AD-70c5e58341ec
    - 원글의 번역본을 참고

- 네트워크의 학습시 시 발생할 수 있는 오류들을 줄이기 위한 프로세스를 정리
- 본문 시작전에 다음의 두 가지 중요 관찰사항에 대해 논함

## 1. 인공신경망의 학습과정은 딥러닝 라이브러리를 이용하여 완벽히 추상화하는것이 불가능함
- 보통의 딥러닝 라이브러리를 이용한 네트워크의 학습은 아래와 같은 흐름을 따름

```
{%raw%}
dataset = my_dataset
model = my_model(my_transform, dataset, ResNet50, SGDOptimizer)
{%endraw%}
```

- 복잡한 실제 연산의 과정을 딥러닝 라이브러리는 위의 코드처럼 쉽게 표현되어있음
  - 하지만 실제로는 그렇지 않음
- 일반적인 딥러닝 모델의 학습에 역전파(back-propagation)와 SGD만 적용한다고 해서 인공신경망이 자동으로 동작하지 않음
  - Batch-norm을 적용한다고 해서 optimization이 마술처럼 이루어지는것도 아니고
  - RNN을 도입한다 해서 텍스트를 마술처럼 자동으로 이해하게 되는것도 아니며
  - 강화학습으로 문제를 정의할 수 있다 하여 꼭 그렇게 풀어야 하는것도 아님

## 2. 학습의 실패는 예고없이 등장함
- 코딩할 때 코드를 잘못 짜거나 설정을 잘못한다면 종종 예외처리문(error)을 만나게 됨
  - 문자열 자리에 정수를 넣거나, 매개변수 갯수가 다르다거나, import가 실패하거나, 혹은 입출력의 모양이 다르거나..
- 인공신경망 학습에 있어서 위와같은 쉬운 오류 외에 어디서 잘못되었는지도 모르는 오류가 발생할 확률이 매우 큼
  - 오류 자체가 구문적이며 단위적으로 테스트하며 디버깅하기가 매우 까다로움
  - 예를들어, 
    - Data augmentation시 이미지의 좌우를 뒤집으면서 lable도 뒤집는것을 깜빡해서 인공신경망이 이러한 잘못된 정보까지 학습하는 바람에 오히려 모델의 정확도가 향상 될 가능성도 존재하게 됨.
    - 자기회기모델(auto-regressive model)을 학습시키며 off-by-one bug(배열이나 루프에서 바운더리 컨디션 체크와 관련한 논리적 에러)로 인해 예측하려는 값을 input으로 취하는 실수
    - Gradient를 잘라낸다는게 loss를 잘라내서 학습과정에서 outlier data들이 무시되게 될 수도 있음
    - 미리 학습해둔 checkpoint로부터 weight를 초기화했는데 해당 평균값을 쓰지 않는 실수
    - 정규화 세팅, 학습률, 학습률 감소율, 모델 크기 등에 대한 설정 잘못
  - 위의 경우 외에도 어떠한 잘못된 설정으로 네트워크를 학습시켰을 때 정확도가 좋아질 수 있고, 이는 순전히 운이 좋은경우에 해당
  - 대부분의 경우 학습은 문제없다는듯이 잘 진행되며 성능만 살짝 덜어지게 됨

- 이로인해 __신경망을 학습하기 위한 방법으로 빠르고 강력한 방법은 전혀 효과적이지 않고__ 오히려 학습에 어려움을 줄 수 있음
  - 네트워크를 빠르게 학습시키는 여러 방법들이 실제로는 효용성이 떨어진다는 의미로 판단됨.
- 따라서 네트워크의 학습에는 조심스럽고 방어적이고 시각화를 중요하게 여기는 방향으로 접근해야 학습에 좀 더 효과적임
  - 글쓴이의 경험에 따르면.. 참을성 있게 디테일에 집착하는 태도가 딥러닝 모델의 학습을 성공시키는데 가장 중요한 요소라고 함.

## 



